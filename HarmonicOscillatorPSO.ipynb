{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzxtDshUsy7xb8uE9xhgov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorvalente/HarmonicOscillatorPSO/blob/main/HarmonicOscillatorPSO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpROX2cmznmg",
        "outputId": "c222f4fe-42e2-4344-f8fe-8708d2fd1f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SPHERE Swarm Intelligence System...\n",
            "Current Time: 2025-03-31 11:44:09\n",
            "Initialized 2 agents.\n",
            "Initialized SwarmQueen.\n",
            "\n",
            "--- Starting Data Loading and Processing ---\n",
            "Target Subject: 00001\n",
            "Attempting to load data from: /content/SPHERE_unzipped\n",
            "Attempting to load accelerometer for subject 00001 from base path: /content/SPHERE_unzipped\n",
            "  Using subject data folder: /content/SPHERE_unzipped/train/00001\n",
            "  Loading data from: /content/SPHERE_unzipped/train/00001/acceleration.csv\n",
            "  Found timestamp column: 't'\n",
            "  Converting timestamp column (assuming units are seconds)...\n",
            "  ...Loaded 35710 rows.\n",
            "Attempting to load pir for subject 00001 from base path: /content/SPHERE_unzipped\n",
            "  Using subject data folder: /content/SPHERE_unzipped/train/00001\n",
            "  Loading data from: /content/SPHERE_unzipped/train/00001/pir.csv\n",
            "  Using 'start' column as timestamp index for pir.\n",
            "  ...Processed 115 pir rows.\n",
            "Attempting to load annotations for subject 00001 from base path: /content/SPHERE_unzipped\n",
            "  Using subject data folder: /content/SPHERE_unzipped/train/00001\n",
            "  Found annotation files: ['annotations_0.csv', 'annotations_1.csv']\n",
            "    Successfully loaded annotations_0.csv\n",
            "    Successfully loaded annotations_1.csv\n",
            "  Concatenated 2 annotation files.\n",
            "  Using 'start' column as timestamp index for annotations.\n",
            "  ...Processed 744 annotations rows.\n",
            "\n",
            "--- Essential data (Accel, Annotations) loaded successfully. Proceeding... ---\n",
            "--- PIR data loaded successfully. ---\n",
            "Synchronizing data streams to 50ms frequency...\n",
            "  Processing ACCELEROMETER...\n",
            "    Resampled ACCELEROMETER to 36478 rows.\n",
            "  Processing PIR...\n",
            "    Resampled PIR to 36327 rows.\n",
            "  Merging streams...\n",
            "    Merged PIR, shape now: (72805, 16)\n",
            "  Forward-filling missing values...\n",
            "  Final synchronized shape: (72805, 16)\n",
            "Sorting annotations index...\n",
            "\n",
            "--- Processing Windows ---\n",
            "Creating time windows (size: 2.0s, overlap: 1.0s)...\n",
            "\n",
            "  [DEBUG GT W1] Window: [1970-01-01 00:00:00, 1970-01-01 00:00:02)\n",
            "  [DEBUG GT W1] Midpoint: 1970-01-01 00:00:01\n",
            "  [DEBUG GT W1] Match Condition Sum: 0\n",
            "\n",
            "  [DEBUG GT W2] Window: [1970-01-01 00:00:01, 1970-01-01 00:00:03)\n",
            "  [DEBUG GT W2] Midpoint: 1970-01-01 00:00:02\n",
            "  [DEBUG GT W2] Match Condition Sum: 0\n",
            "\n",
            "  [DEBUG GT W3] Window: [1970-01-01 00:00:02, 1970-01-01 00:00:04)\n",
            "  [DEBUG GT W3] Midpoint: 1970-01-01 00:00:03\n",
            "  [DEBUG GT W3] Match Condition Sum: 0\n",
            "\n",
            "  [DEBUG GT W4] Window: [1970-01-01 00:00:03, 1970-01-01 00:00:05)\n",
            "  [DEBUG GT W4] Midpoint: 1970-01-01 00:00:04\n",
            "  [DEBUG GT W4] Match Condition Sum: 0\n",
            "\n",
            "  [DEBUG GT W5] Window: [1970-01-01 00:00:04, 1970-01-01 00:00:06)\n",
            "  [DEBUG GT W5] Midpoint: 1970-01-01 00:00:05\n",
            "  [DEBUG GT W5] Match Condition Sum: 0\n",
            "\n",
            "  [DEBUG GT W101] Window: [1970-01-01 00:01:40, 1970-01-01 00:01:42)\n",
            "  [DEBUG GT W101] Midpoint: 1970-01-01 00:01:41\n",
            "  [DEBUG GT W101] Match Condition Sum: 1\n",
            "  [DEBUG GT W101] Matched Annotation Row (Index=1970-01-01 00:01:40.920000):\n",
            "end      1970-01-01 00:01:43.640000\n",
            "name                         p_bent\n",
            "index                             5\n",
            "  [DEBUG GT W101] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W101] Activity Col Found?: name\n",
            "  [DEBUG GT W101] Ground Truth Set To: 'p_bent'\n",
            "  Processed 200 windows... Time: 00:03:21.000 Pred: t_sit_lie (GT: p_lie) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W201] Window: [1970-01-01 00:03:20, 1970-01-01 00:03:22)\n",
            "  [DEBUG GT W201] Midpoint: 1970-01-01 00:03:21\n",
            "  [DEBUG GT W201] Match Condition Sum: 2\n",
            "  [DEBUG GT W201] Matched Annotation Row (Index=1970-01-01 00:03:19.500000):\n",
            "end      1970-01-01 00:03:36.780000\n",
            "name                          p_lie\n",
            "index                             7\n",
            "  [DEBUG GT W201] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W201] Activity Col Found?: name\n",
            "  [DEBUG GT W201] Ground Truth Set To: 'p_lie'\n",
            "\n",
            "  [DEBUG GT W301] Window: [1970-01-01 00:05:00, 1970-01-01 00:05:02)\n",
            "  [DEBUG GT W301] Midpoint: 1970-01-01 00:05:01\n",
            "  [DEBUG GT W301] Match Condition Sum: 2\n",
            "  [DEBUG GT W301] Matched Annotation Row (Index=1970-01-01 00:05:00.040000):\n",
            "end      1970-01-01 00:05:02.040000\n",
            "name                         p_bent\n",
            "index                             5\n",
            "  [DEBUG GT W301] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W301] Activity Col Found?: name\n",
            "  [DEBUG GT W301] Ground Truth Set To: 'p_bent'\n",
            "  Processed 400 windows... Time: 00:06:41.000 Pred: t_sit_lie (GT: p_lie) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W401] Window: [1970-01-01 00:06:40, 1970-01-01 00:06:42)\n",
            "  [DEBUG GT W401] Midpoint: 1970-01-01 00:06:41\n",
            "  [DEBUG GT W401] Match Condition Sum: 2\n",
            "  [DEBUG GT W401] Matched Annotation Row (Index=1970-01-01 00:06:30.790000):\n",
            "end      1970-01-01 00:06:46.790000\n",
            "name                          p_lie\n",
            "index                             7\n",
            "  [DEBUG GT W401] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W401] Activity Col Found?: name\n",
            "  [DEBUG GT W401] Ground Truth Set To: 'p_lie'\n",
            "\n",
            "  [DEBUG GT W501] Window: [1970-01-01 00:08:20, 1970-01-01 00:08:22)\n",
            "  [DEBUG GT W501] Midpoint: 1970-01-01 00:08:21\n",
            "  [DEBUG GT W501] Match Condition Sum: 2\n",
            "  [DEBUG GT W501] Matched Annotation Row (Index=1970-01-01 00:08:15.700000):\n",
            "end      1970-01-01 00:08:21.700000\n",
            "name                        p_stand\n",
            "index                            10\n",
            "  [DEBUG GT W501] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W501] Activity Col Found?: name\n",
            "  [DEBUG GT W501] Ground Truth Set To: 'p_stand'\n",
            "  Processed 600 windows... Time: 00:10:01.000 Pred: a_walk (GT: a_walk) Loc: kitchen\n",
            "\n",
            "  [DEBUG GT W601] Window: [1970-01-01 00:10:00, 1970-01-01 00:10:02)\n",
            "  [DEBUG GT W601] Midpoint: 1970-01-01 00:10:01\n",
            "  [DEBUG GT W601] Match Condition Sum: 2\n",
            "  [DEBUG GT W601] Matched Annotation Row (Index=1970-01-01 00:09:58.611000):\n",
            "end      1970-01-01 00:10:01.020000\n",
            "name                         a_walk\n",
            "index                             4\n",
            "  [DEBUG GT W601] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W601] Activity Col Found?: name\n",
            "  [DEBUG GT W601] Ground Truth Set To: 'a_walk'\n",
            "\n",
            "  [DEBUG GT W701] Window: [1970-01-01 00:11:40, 1970-01-01 00:11:42)\n",
            "  [DEBUG GT W701] Midpoint: 1970-01-01 00:11:41\n",
            "  [DEBUG GT W701] Match Condition Sum: 2\n",
            "  [DEBUG GT W701] Matched Annotation Row (Index=1970-01-01 00:11:20.790000):\n",
            "end      1970-01-01 00:11:43.790000\n",
            "name                        p_stand\n",
            "index                            10\n",
            "  [DEBUG GT W701] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W701] Activity Col Found?: name\n",
            "  [DEBUG GT W701] Ground Truth Set To: 'p_stand'\n",
            "  Processed 800 windows... Time: 00:13:21.000 Pred: a_walk (GT: p_sit) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W801] Window: [1970-01-01 00:13:20, 1970-01-01 00:13:22)\n",
            "  [DEBUG GT W801] Midpoint: 1970-01-01 00:13:21\n",
            "  [DEBUG GT W801] Match Condition Sum: 2\n",
            "  [DEBUG GT W801] Matched Annotation Row (Index=1970-01-01 00:12:23.330000):\n",
            "end      1970-01-01 00:16:04.730000\n",
            "name                          p_sit\n",
            "index                             8\n",
            "  [DEBUG GT W801] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W801] Activity Col Found?: name\n",
            "  [DEBUG GT W801] Ground Truth Set To: 'p_sit'\n",
            "\n",
            "  [DEBUG GT W901] Window: [1970-01-01 00:15:00, 1970-01-01 00:15:02)\n",
            "  [DEBUG GT W901] Midpoint: 1970-01-01 00:15:01\n",
            "  [DEBUG GT W901] Match Condition Sum: 2\n",
            "  [DEBUG GT W901] Matched Annotation Row (Index=1970-01-01 00:12:23.330000):\n",
            "end      1970-01-01 00:16:04.730000\n",
            "name                          p_sit\n",
            "index                             8\n",
            "  [DEBUG GT W901] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W901] Activity Col Found?: name\n",
            "  [DEBUG GT W901] Ground Truth Set To: 'p_sit'\n",
            "  Processed 1000 windows... Time: 00:16:41.000 Pred: a_walk (GT: p_stand) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W1001] Window: [1970-01-01 00:16:40, 1970-01-01 00:16:42)\n",
            "  [DEBUG GT W1001] Midpoint: 1970-01-01 00:16:41\n",
            "  [DEBUG GT W1001] Match Condition Sum: 2\n",
            "  [DEBUG GT W1001] Matched Annotation Row (Index=1970-01-01 00:16:29.814000):\n",
            "end      1970-01-01 00:18:02.814000\n",
            "name                        p_stand\n",
            "index                            10\n",
            "  [DEBUG GT W1001] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1001] Activity Col Found?: name\n",
            "  [DEBUG GT W1001] Ground Truth Set To: 'p_stand'\n",
            "\n",
            "  [DEBUG GT W1101] Window: [1970-01-01 00:18:20, 1970-01-01 00:18:22)\n",
            "  [DEBUG GT W1101] Midpoint: 1970-01-01 00:18:21\n",
            "  [DEBUG GT W1101] Match Condition Sum: 2\n",
            "  [DEBUG GT W1101] Matched Annotation Row (Index=1970-01-01 00:18:20.130000):\n",
            "end      1970-01-01 00:18:24.230000\n",
            "name                       a_ascend\n",
            "index                             0\n",
            "  [DEBUG GT W1101] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1101] Activity Col Found?: name\n",
            "  [DEBUG GT W1101] Ground Truth Set To: 'a_ascend'\n",
            "  Processed 1200 windows... Time: 00:20:01.000 Pred: a_walk (GT: t_kneel_stand) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W1201] Window: [1970-01-01 00:20:00, 1970-01-01 00:20:02)\n",
            "  [DEBUG GT W1201] Midpoint: 1970-01-01 00:20:01\n",
            "  [DEBUG GT W1201] Match Condition Sum: 2\n",
            "  [DEBUG GT W1201] Matched Annotation Row (Index=1970-01-01 00:19:58.644000):\n",
            "end      1970-01-01 00:20:03.644000\n",
            "name                        p_stand\n",
            "index                            10\n",
            "  [DEBUG GT W1201] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1201] Activity Col Found?: name\n",
            "  [DEBUG GT W1201] Ground Truth Set To: 'p_stand'\n",
            "\n",
            "  [DEBUG GT W1301] Window: [1970-01-01 00:21:40, 1970-01-01 00:21:42)\n",
            "  [DEBUG GT W1301] Midpoint: 1970-01-01 00:21:41\n",
            "  [DEBUG GT W1301] Match Condition Sum: 2\n",
            "  [DEBUG GT W1301] Matched Annotation Row (Index=1970-01-01 00:21:39.834000):\n",
            "end      1970-01-01 00:21:43.154000\n",
            "name                        p_squat\n",
            "index                             9\n",
            "  [DEBUG GT W1301] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1301] Activity Col Found?: name\n",
            "  [DEBUG GT W1301] Ground Truth Set To: 'p_squat'\n",
            "  Processed 1400 windows... Time: 00:23:21.000 Pred: t_sit_lie (GT: t_lie_sit) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W1401] Window: [1970-01-01 00:23:20, 1970-01-01 00:23:22)\n",
            "  [DEBUG GT W1401] Midpoint: 1970-01-01 00:23:21\n",
            "  [DEBUG GT W1401] Match Condition Sum: 2\n",
            "  [DEBUG GT W1401] Matched Annotation Row (Index=1970-01-01 00:23:17.884000):\n",
            "end      1970-01-01 00:23:21.024000\n",
            "name                      t_lie_sit\n",
            "index                            13\n",
            "  [DEBUG GT W1401] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1401] Activity Col Found?: name\n",
            "  [DEBUG GT W1401] Ground Truth Set To: 't_lie_sit'\n",
            "\n",
            "  [DEBUG GT W1501] Window: [1970-01-01 00:25:00, 1970-01-01 00:25:02)\n",
            "  [DEBUG GT W1501] Midpoint: 1970-01-01 00:25:01\n",
            "  [DEBUG GT W1501] Match Condition Sum: 2\n",
            "  [DEBUG GT W1501] Matched Annotation Row (Index=1970-01-01 00:24:55.378000):\n",
            "end      1970-01-01 00:25:06.967000\n",
            "name                          p_sit\n",
            "index                             8\n",
            "  [DEBUG GT W1501] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1501] Activity Col Found?: name\n",
            "  [DEBUG GT W1501] Ground Truth Set To: 'p_sit'\n",
            "  Processed 1600 windows... Time: 00:26:41.000 Pred: a_walk (GT: p_stand) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W1601] Window: [1970-01-01 00:26:40, 1970-01-01 00:26:42)\n",
            "  [DEBUG GT W1601] Midpoint: 1970-01-01 00:26:41\n",
            "  [DEBUG GT W1601] Match Condition Sum: 2\n",
            "  [DEBUG GT W1601] Matched Annotation Row (Index=1970-01-01 00:26:37.504000):\n",
            "end      1970-01-01 00:26:42.614000\n",
            "name                        p_stand\n",
            "index                            10\n",
            "  [DEBUG GT W1601] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1601] Activity Col Found?: name\n",
            "  [DEBUG GT W1601] Ground Truth Set To: 'p_stand'\n",
            "\n",
            "  [DEBUG GT W1701] Window: [1970-01-01 00:28:20, 1970-01-01 00:28:22)\n",
            "  [DEBUG GT W1701] Midpoint: 1970-01-01 00:28:21\n",
            "  [DEBUG GT W1701] Match Condition Sum: 2\n",
            "  [DEBUG GT W1701] Matched Annotation Row (Index=1970-01-01 00:28:00.606000):\n",
            "end      1970-01-01 00:28:39.665000\n",
            "name                        p_stand\n",
            "index                            10\n",
            "  [DEBUG GT W1701] Available columns in annotations df: ['end', 'name', 'index']\n",
            "  [DEBUG GT W1701] Activity Col Found?: name\n",
            "  [DEBUG GT W1701] Ground Truth Set To: 'p_stand'\n",
            "  Processed 1800 windows... Time: 00:30:01.000 Pred: a_walk (GT: N/A) Loc: UNKNOWN\n",
            "\n",
            "  [DEBUG GT W1801] Window: [1970-01-01 00:30:00, 1970-01-01 00:30:02)\n",
            "  [DEBUG GT W1801] Midpoint: 1970-01-01 00:30:01\n",
            "  [DEBUG GT W1801] Match Condition Sum: 0\n",
            "Generated 1822 windows.\n",
            "\n",
            "--- Finished processing 1822 windows in 15.39 seconds ---\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Collected 1751 predictions with valid ground truth.\n",
            "\n",
            "Overall Accuracy: 0.1085\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     a_ascend       0.00      0.00      0.00        19\n",
            "    a_descend       0.00      0.00      0.00        15\n",
            "       a_jump       0.00      0.00      0.00         3\n",
            "       a_walk       0.14      1.00      0.25       169\n",
            "       p_bent       0.00      0.00      0.00        53\n",
            "      p_kneel       0.00      0.00      0.00        29\n",
            "        p_lie       0.00      0.00      0.00       168\n",
            "        p_sit       0.00      0.00      0.00       312\n",
            "      p_squat       0.00      0.00      0.00        12\n",
            "      p_stand       0.00      0.00      0.00       707\n",
            "       t_bend       0.00      0.00      0.00        20\n",
            "t_kneel_stand       0.00      0.00      0.00        11\n",
            "    t_lie_sit       0.00      0.00      0.00        23\n",
            "    t_sit_lie       0.04      0.91      0.08        23\n",
            "  t_sit_stand       0.00      0.00      0.00        27\n",
            "t_stand_kneel       0.00      0.00      0.00        11\n",
            "  t_stand_sit       0.00      0.00      0.00        35\n",
            " t_straighten       0.00      0.00      0.00        28\n",
            "       t_turn       0.00      0.00      0.00        86\n",
            "\n",
            "     accuracy                           0.11      1751\n",
            "    macro avg       0.01      0.10      0.02      1751\n",
            " weighted avg       0.01      0.11      0.02      1751\n",
            "\n",
            "\n",
            "Final Confusion Matrix:\n",
            "               a_ascend  a_descend  a_jump  a_walk  p_bent  p_kneel  p_lie  \\\n",
            "a_ascend            0.0        0.0     0.0    19.0     0.0      0.0    0.0   \n",
            "a_descend           0.0        0.0     0.0    15.0     0.0      0.0    0.0   \n",
            "a_jump              0.0        0.0     0.0     3.0     0.0      0.0    0.0   \n",
            "a_walk              0.0        0.0     0.0   169.0     0.0      0.0    0.0   \n",
            "p_bent              0.0        0.0     0.0    41.0     0.0      0.0    0.0   \n",
            "p_kneel             0.0        0.0     0.0    11.0     0.0      0.0    0.0   \n",
            "p_lie               0.0        0.0     0.0     5.0     0.0      0.0    0.0   \n",
            "p_sit               0.0        0.0     0.0   232.0     0.0      0.0    0.0   \n",
            "p_squat             0.0        0.0     0.0     0.0     0.0      0.0    0.0   \n",
            "p_stand             2.0        0.0     0.0   546.0     0.0      0.0    0.0   \n",
            "t_bend              0.0        0.0     0.0    10.0     0.0      0.0    0.0   \n",
            "t_kneel_stand       0.0        0.0     0.0     6.0     0.0      0.0    0.0   \n",
            "t_lie_sit           0.0        0.0     0.0     0.0     0.0      0.0    0.0   \n",
            "t_sit_lie           0.0        0.0     0.0     2.0     0.0      0.0    0.0   \n",
            "t_sit_stand         0.0        0.0     0.0    13.0     0.0      0.0    0.0   \n",
            "t_stand_kneel       0.0        0.0     0.0     2.0     0.0      0.0    0.0   \n",
            "t_stand_sit         0.0        0.0     0.0    34.0     0.0      0.0    0.0   \n",
            "t_straighten        0.0        0.0     0.0    17.0     0.0      0.0    0.0   \n",
            "t_turn              0.0        0.0     0.0    84.0     0.0      0.0    0.0   \n",
            "\n",
            "               p_sit  p_squat  p_stand  t_bend  t_kneel_stand  t_lie_sit  \\\n",
            "a_ascend         0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "a_descend        0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "a_jump           0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "a_walk           0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "p_bent           0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "p_kneel          0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "p_lie            0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "p_sit            0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "p_squat          0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "p_stand          8.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_bend           0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_kneel_stand    0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_lie_sit        0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_sit_lie        0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_sit_stand      0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_stand_kneel    0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_stand_sit      0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_straighten     0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "t_turn           0.0      0.0      0.0     0.0            0.0        0.0   \n",
            "\n",
            "               t_sit_lie  t_sit_stand  t_stand_kneel  t_stand_sit  \\\n",
            "a_ascend             0.0          0.0            0.0          0.0   \n",
            "a_descend            0.0          0.0            0.0          0.0   \n",
            "a_jump               0.0          0.0            0.0          0.0   \n",
            "a_walk               0.0          0.0            0.0          0.0   \n",
            "p_bent              12.0          0.0            0.0          0.0   \n",
            "p_kneel             18.0          0.0            0.0          0.0   \n",
            "p_lie              163.0          0.0            0.0          0.0   \n",
            "p_sit               80.0          0.0            0.0          0.0   \n",
            "p_squat             12.0          0.0            0.0          0.0   \n",
            "p_stand            146.0          0.0            0.0          0.0   \n",
            "t_bend              10.0          0.0            0.0          0.0   \n",
            "t_kneel_stand        5.0          0.0            0.0          0.0   \n",
            "t_lie_sit           23.0          0.0            0.0          0.0   \n",
            "t_sit_lie           21.0          0.0            0.0          0.0   \n",
            "t_sit_stand         14.0          0.0            0.0          0.0   \n",
            "t_stand_kneel        9.0          0.0            0.0          0.0   \n",
            "t_stand_sit          1.0          0.0            0.0          0.0   \n",
            "t_straighten        11.0          0.0            0.0          0.0   \n",
            "t_turn               2.0          0.0            0.0          0.0   \n",
            "\n",
            "               t_straighten  t_turn  \n",
            "a_ascend                0.0     0.0  \n",
            "a_descend               0.0     0.0  \n",
            "a_jump                  0.0     0.0  \n",
            "a_walk                  0.0     0.0  \n",
            "p_bent                  0.0     0.0  \n",
            "p_kneel                 0.0     0.0  \n",
            "p_lie                   0.0     0.0  \n",
            "p_sit                   0.0     0.0  \n",
            "p_squat                 0.0     0.0  \n",
            "p_stand                 0.0     5.0  \n",
            "t_bend                  0.0     0.0  \n",
            "t_kneel_stand           0.0     0.0  \n",
            "t_lie_sit               0.0     0.0  \n",
            "t_sit_lie               0.0     0.0  \n",
            "t_sit_stand             0.0     0.0  \n",
            "t_stand_kneel           0.0     0.0  \n",
            "t_stand_sit             0.0     0.0  \n",
            "t_straighten            0.0     0.0  \n",
            "t_turn                  0.0     0.0  \n",
            "\n",
            "Confusion matrix plot saved as confusion_matrix_00001.png\n",
            "\n",
            "System processing finished (or terminated due to loading/sync error).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from enum import Enum, auto\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from collections import defaultdict, deque\n",
        "import time\n",
        "from scipy.fft import rfft, rfftfreq # For frequency domain features\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration & Enums ---\n",
        "\n",
        "class SensorType(Enum):\n",
        "    ACCELEROMETER = auto()\n",
        "    PIR = auto()\n",
        "    RSSI = auto()\n",
        "    VIDEO = auto()\n",
        "\n",
        "class ActivityType(Enum):\n",
        "    AMBULATION = auto()\n",
        "    POSTURE = auto()\n",
        "    TRANSITION = auto()\n",
        "\n",
        "class Location(Enum):\n",
        "    BATH = auto()\n",
        "    BED1 = auto()\n",
        "    BED2 = auto()\n",
        "    HALL = auto()\n",
        "    KITCHEN = auto()\n",
        "    LIVING = auto()\n",
        "    STAIRS = auto()\n",
        "    STUDY = auto()\n",
        "    TOILET = auto()\n",
        "    UNKNOWN = auto()\n",
        "\n",
        "SPHERE_ACTIVITIES = {\n",
        "    'a_ascend': ActivityType.AMBULATION, 'a_descend': ActivityType.AMBULATION,\n",
        "    'a_jump': ActivityType.AMBULATION, 'a_loadwalk': ActivityType.AMBULATION,\n",
        "    'a_walk': ActivityType.AMBULATION, 'p_bent': ActivityType.POSTURE,\n",
        "    'p_kneel': ActivityType.POSTURE, 'p_lie': ActivityType.POSTURE,\n",
        "    'p_sit': ActivityType.POSTURE, 'p_squat': ActivityType.POSTURE,\n",
        "    'p_stand': ActivityType.POSTURE, 't_bend': ActivityType.TRANSITION,\n",
        "    't_kneel_stand': ActivityType.TRANSITION, 't_lie_sit': ActivityType.TRANSITION,\n",
        "    't_sit_lie': ActivityType.TRANSITION, 't_sit_stand': ActivityType.TRANSITION,\n",
        "    't_stand_kneel': ActivityType.TRANSITION, 't_stand_sit': ActivityType.TRANSITION,\n",
        "    't_straighten': ActivityType.TRANSITION, 't_turn': ActivityType.TRANSITION\n",
        "}\n",
        "SPHERE_ACTIVITY_NAMES = list(SPHERE_ACTIVITIES.keys())\n",
        "\n",
        "SPHERE_LOCATIONS = ['bath', 'bed1', 'bed2', 'hall', 'kitchen', 'living', 'stairs', 'study', 'toilet']\n",
        "SPHERE_LOCATION_ENUM_MAP = {name: Location[name.upper()] for name in SPHERE_LOCATIONS}\n",
        "SPHERE_LOCATION_ENUM_MAP['UNKNOWN'] = Location.UNKNOWN\n",
        "\n",
        "# --- Data Handling Utilities (Revised v4 - Step 1) ---\n",
        "\n",
        "def load_sphere_data(data_path, subject_id, data_type):\n",
        "    \"\"\"\n",
        "    Loads specific sensor data or annotations for a subject,\n",
        "    adapted for potential SPHERE challenge structures (v4 - handles PIR start/end).\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load {data_type} for subject {subject_id} from base path: {data_path}\")\n",
        "    file_path = None # Used for logging the primary file path loaded\n",
        "    df = None      # DataFrame to be loaded\n",
        "\n",
        "    # --- Path and Filename Logic ---\n",
        "    data_type_lower = data_type.lower()\n",
        "\n",
        "    # Determine base subject folder (try train, test, then base)\n",
        "    subject_folder_options = [\n",
        "        os.path.join(data_path, 'train', subject_id),\n",
        "        os.path.join(data_path, 'test', subject_id),\n",
        "        os.path.join(data_path, subject_id)\n",
        "    ]\n",
        "    subject_base_path = None\n",
        "    for folder_option in subject_folder_options:\n",
        "        if os.path.isdir(folder_option):\n",
        "            subject_base_path = folder_option\n",
        "            print(f\"  Using subject data folder: {subject_base_path}\")\n",
        "            break\n",
        "\n",
        "    if subject_base_path is None:\n",
        "         if data_type_lower == 'annotations':\n",
        "              print(\"  Subject folder not found, checking for annotations in base/train paths...\")\n",
        "              pass\n",
        "         else:\n",
        "              print(f\"Error: Could not find subject folder for {subject_id} in train/, test/, or base directory relative to {data_path}.\")\n",
        "              return None\n",
        "\n",
        "    # --- Specific Loading Logic per Type ---\n",
        "    if data_type_lower == 'annotations':\n",
        "        annotation_files = []\n",
        "        if subject_base_path:\n",
        "             annotation_pattern = os.path.join(subject_base_path, 'annotations_*.csv')\n",
        "             annotation_files = sorted(glob.glob(annotation_pattern))\n",
        "        if not annotation_files:\n",
        "             print(f\"  Annotations not found in subject folder, checking other locations...\")\n",
        "             possible_paths = [ os.path.join(data_path,'train',f'{subject_id}_annotations.csv'), os.path.join(data_path,'train','annotations.csv'),\n",
        "                                os.path.join(data_path,'annotations',f'{subject_id}.csv'), os.path.join(data_path,'annotations.csv') ]\n",
        "             for p in possible_paths:\n",
        "                 if os.path.exists(p): annotation_files = [p]; print(f\"  Found potential annotation file at: {p}\"); break\n",
        "        if not annotation_files: print(f\"Error: No annotation files found for subject {subject_id}.\"); return None\n",
        "\n",
        "        print(f\"  Found annotation files: { [os.path.basename(f) for f in annotation_files] }\")\n",
        "        file_path = annotation_files[0]\n",
        "        all_annotations_df = []\n",
        "        for f in annotation_files:\n",
        "             try: df_part = pd.read_csv(f); all_annotations_df.append(df_part); print(f\"    Successfully loaded {os.path.basename(f)}\")\n",
        "             except Exception as e: print(f\"Warning: Could not load or read annotation file {f}: {e}\")\n",
        "        if not all_annotations_df: print(f\"Error: Failed to load any valid data from found annotation files.\"); return None\n",
        "        df = pd.concat(all_annotations_df, ignore_index=True); print(f\"  Concatenated {len(annotation_files)} annotation files.\")\n",
        "    else:\n",
        "        if subject_base_path is None: return None\n",
        "        filename_map = {'accelerometer':['acceleration.csv','acc*.csv'], 'pir':['pir.csv'], 'rssi':['rssi*.csv'], 'video_features':['video_features*.csv']}\n",
        "        patterns = filename_map.get(data_type_lower)\n",
        "        if not patterns: print(f\"Error: Unknown data_type '{data_type_lower}'.\"); return None\n",
        "        found_files = []\n",
        "        for pattern in patterns:\n",
        "             matching_files = glob.glob(os.path.join(subject_base_path, pattern))\n",
        "             if matching_files: found_files.extend(matching_files)\n",
        "        if not found_files: print(f\"Warning: No file matching '{patterns}' for {data_type_lower} in {subject_base_path}\"); return None\n",
        "        primary_target = os.path.join(subject_base_path, patterns[0])\n",
        "        file_path = primary_target if primary_target in found_files else sorted(found_files)[0]\n",
        "        if file_path != primary_target: print(f\"  Note: Using first matched file: {os.path.basename(file_path)}\")\n",
        "        print(f\"  Loading data from: {file_path}\")\n",
        "        try: df = pd.read_csv(file_path)\n",
        "        except Exception as e: print(f\"Error loading sensor data file {file_path}: {e}\"); traceback.print_exc(); return None\n",
        "\n",
        "    # --- Common Processing (Timestamping, Indexing) ---\n",
        "    if df is None: print(f\"Error: DataFrame is None for {data_type_lower}.\"); return None\n",
        "    try:\n",
        "        timestamp_cols_to_try = ['t', 'timestamp', 'time']\n",
        "        timestamp_col = None\n",
        "        for col in timestamp_cols_to_try:\n",
        "            if col in df.columns: timestamp_col = col; print(f\"  Found timestamp column: '{timestamp_col}'\"); break\n",
        "\n",
        "        if timestamp_col is None and (data_type_lower == 'annotations' or data_type_lower == 'pir'):\n",
        "            if 'start' in df.columns and 'end' in df.columns:\n",
        "                 timestamp_col = 'start'; print(f\"  Using 'start' column as timestamp index for {data_type_lower}.\")\n",
        "                 df['start'] = pd.to_datetime(df['start'], unit='s'); df['end'] = pd.to_datetime(df['end'], unit='s') # <<< VERIFY UNIT\n",
        "                 df = df.set_index('start').sort_index(); print(f\"  ...Processed {len(df)} {data_type_lower} rows.\"); return df\n",
        "            else: print(f\"Error: {data_type_lower.capitalize()} file lacks 'start'/'end' and standard timestamp columns.\"); return None\n",
        "        elif timestamp_col is None: print(f\"Error: Could not find suitable timestamp column {timestamp_cols_to_try} in {file_path}\"); return None\n",
        "\n",
        "        if timestamp_col != 'timestamp': df = df.rename(columns={timestamp_col: 'timestamp'})\n",
        "        print(f\"  Converting timestamp column (assuming units are seconds)...\"); df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s') # <<< VERIFY UNIT\n",
        "        df = df.set_index('timestamp').sort_index(); print(f\"  ...Loaded {len(df)} rows.\"); return df\n",
        "    except Exception as e: print(f\"Error processing DataFrame for {file_path if file_path else data_type}: {e}\"); traceback.print_exc(); return None\n",
        "\n",
        "\n",
        "def synchronize_data(data_streams, resample_freq='50ms'):\n",
        "    \"\"\"Synchronizes multiple data streams via resampling and forward-fill.\"\"\"\n",
        "    print(f\"Synchronizing data streams to {resample_freq} frequency...\")\n",
        "    combined_df = None; processed_streams = {}\n",
        "    for sensor_type, df in data_streams.items():\n",
        "        if df is None or df.empty: print(f\"  Skipping empty stream: {sensor_type.name}\"); continue\n",
        "        print(f\"  Processing {sensor_type.name}...\")\n",
        "        if not isinstance(df.index, pd.DatetimeIndex): print(f\"Error: DataFrame for {sensor_type.name} lacks DatetimeIndex.\"); continue\n",
        "        try:\n",
        "            if sensor_type == SensorType.PIR:\n",
        "                if 'end' in df.columns:\n",
        "                     min_time=df.index.min(); max_time=df['end'].max()\n",
        "                     if pd.isna(min_time) or pd.isna(max_time): print(f\"Warning: Invalid min/max time for PIR {sensor_type.name}. Skipping.\"); continue\n",
        "                     target_index=pd.date_range(start=min_time,end=max_time,freq=resample_freq)\n",
        "                     resampled_df_pir=pd.DataFrame(0,index=target_index,columns=SPHERE_LOCATIONS)\n",
        "                     for start, row in df.iterrows():\n",
        "                          end=row['end']; active_indices=target_index[(target_index>=start)&(target_index<end)]\n",
        "                          if 'name' in row.index and row['name'] in SPHERE_LOCATIONS: resampled_df_pir.loc[active_indices,row['name']]=1\n",
        "                     resampled_df=resampled_df_pir.fillna(0)\n",
        "                else: print(f\"Warning: Cannot resample PIR {sensor_type.name} without 'end' column. Skipping.\"); continue\n",
        "            else: resampled_df = df.resample(resample_freq).mean()\n",
        "            resampled_df.columns=[f\"{col}_{sensor_type.name}\" for col in resampled_df.columns]; processed_streams[sensor_type]=resampled_df; print(f\"    Resampled {sensor_type.name} to {len(resampled_df)} rows.\")\n",
        "        except Exception as e: print(f\"Error resampling {sensor_type.name}: {e}\"); traceback.print_exc()\n",
        "    print(\"  Merging streams...\")\n",
        "    first_stream = True\n",
        "    for sensor_type, resampled_df in processed_streams.items():\n",
        "        if first_stream: combined_df=resampled_df; first_stream=False\n",
        "        else: combined_df=pd.merge(combined_df,resampled_df,left_index=True,right_index=True,how='outer'); print(f\"    Merged {sensor_type.name}, shape now: {combined_df.shape}\")\n",
        "    if combined_df is not None and not combined_df.empty:\n",
        "        print(\"  Forward-filling missing values...\"); combined_df=combined_df.ffill(); combined_df=combined_df.bfill(); combined_df=combined_df.fillna(0); print(f\"  Final synchronized shape: {combined_df.shape}\")\n",
        "    elif combined_df is not None and combined_df.empty: print(\"Warning: Combined DataFrame is empty after merging.\")\n",
        "    else: print(\"Error: No data streams could be processed and merged.\"); return None\n",
        "    return combined_df\n",
        "\n",
        "def create_time_windows(sync_data, window_size_sec, overlap_sec):\n",
        "    \"\"\"Creates time-windowed chunks from synchronized data.\"\"\"\n",
        "    if sync_data is None or sync_data.empty: print(\"Error: Cannot create windows from empty sync data.\"); return\n",
        "    print(f\"Creating time windows (size: {window_size_sec}s, overlap: {overlap_sec}s)...\")\n",
        "    window_delta=pd.Timedelta(seconds=window_size_sec); step_delta=pd.Timedelta(seconds=window_size_sec-overlap_sec)\n",
        "    if step_delta.total_seconds()<=0: print(\"Error: Overlap >= window size.\"); return\n",
        "    start_time=sync_data.index.min(); end_time=sync_data.index.max(); current_time=start_time; window_count=0\n",
        "    while current_time + window_delta <= end_time:\n",
        "        window_end=current_time+window_delta; window_df=sync_data[(sync_data.index>=current_time)&(sync_data.index<window_end)]\n",
        "        if window_df.empty: current_time+=step_delta; continue\n",
        "        window_chunks={}\n",
        "        for sensor_type in SensorType:\n",
        "            sensor_suffix=f\"_{sensor_type.name}\"; sensor_cols_with_suffix=[col for col in window_df.columns if col.endswith(sensor_suffix)]\n",
        "            if sensor_cols_with_suffix:\n",
        "                sensor_chunk=window_df[sensor_cols_with_suffix].copy(); sensor_chunk.columns=[col.replace(sensor_suffix,'') for col in sensor_cols_with_suffix]\n",
        "                if sensor_type == SensorType.PIR:\n",
        "                    for loc in SPHERE_LOCATIONS:\n",
        "                        if loc not in sensor_chunk.columns: sensor_chunk[loc]=0\n",
        "                    if all(loc in sensor_chunk.columns for loc in SPHERE_LOCATIONS): window_chunks[sensor_type]=sensor_chunk[SPHERE_LOCATIONS]\n",
        "                    else: print(f\"Warning: PIR chunk missing location columns at {window_end_time}\"); window_chunks[sensor_type]=sensor_chunk\n",
        "                else: window_chunks[sensor_type]=sensor_chunk\n",
        "        yield {'timestamp':window_end,'data':window_chunks}; window_count+=1; current_time+=step_delta\n",
        "    print(f\"Generated {window_count} windows.\")\n",
        "\n",
        "\n",
        "# --- Agent Class ---\n",
        "class Agent:\n",
        "    \"\"\"An agent with unique sensory capabilities analyzing sensor data.\"\"\"\n",
        "    def __init__(self, agent_id, sensor_type, feature_extractors=None, learning_rate=0.1, history_len=10):\n",
        "        self.id=agent_id; self.sensor_type=sensor_type; self.learning_rate=learning_rate; self.history=deque(maxlen=history_len)\n",
        "        self.confidence=0.5; self.activity_accuracy={act:0.5 for act in SPHERE_ACTIVITY_NAMES}; self.knowledge={act:1.0/len(SPHERE_ACTIVITY_NAMES) for act in SPHERE_ACTIVITY_NAMES}; self.activity_specialization={act:0.5 for act in SPHERE_ACTIVITY_NAMES}; self.transition_model={}\n",
        "        self.feature_extractors=feature_extractors or self._default_feature_extractors(); self.last_timestamp=None\n",
        "\n",
        "    def _default_feature_extractors(self):\n",
        "        global RESAMPLE_FREQ\n",
        "        try: sampling_rate=1.0/pd.Timedelta(RESAMPLE_FREQ).total_seconds()\n",
        "        except NameError: sampling_rate=20.0; print(f\"Warning: Assuming sampling rate {sampling_rate} Hz.\")\n",
        "        if self.sensor_type==SensorType.ACCELEROMETER:\n",
        "            return {'mean':lambda x:np.nanmean(x,axis=0) if x.shape[0]>0 else np.zeros(3),'std':lambda x:np.nanstd(x,axis=0) if x.shape[0]>0 else np.zeros(3),'max':lambda x:np.nanmax(x,axis=0) if x.shape[0]>0 else np.zeros(3),'min':lambda x:np.nanmin(x,axis=0) if x.shape[0]>0 else np.zeros(3),\n",
        "                    'range':lambda x:np.ptp(x[~np.isnan(x).any(axis=1)],axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>0 else np.zeros(3),'median':lambda x:np.nanmedian(x,axis=0) if x.shape[0]>0 else np.zeros(3),\n",
        "                    'energy':lambda x:np.sum(np.square(x[~np.isnan(x).any(axis=1)]),axis=0)/x.shape[0] if x.shape[0]>0 else np.zeros(3),\n",
        "                    'iqr':lambda x:np.subtract(*np.nanpercentile(x[~np.isnan(x).any(axis=1)],[75,25],axis=0)) if x.shape[0]>0 and np.sum(~np.isnan(x))>1 else np.zeros(3),\n",
        "                    'fft_energy':lambda x:self._calculate_fft_energy(x,sampling_rate) if x.shape[0]>1 else np.zeros(3),'fft_peak_freq':lambda x:self._calculate_fft_peak_freq(x,sampling_rate) if x.shape[0]>1 else np.zeros(3),}\n",
        "        elif self.sensor_type==SensorType.PIR:\n",
        "             return {'sum':lambda x:np.nansum(x.values,axis=0) if x.shape[0]>0 else np.zeros(len(SPHERE_LOCATIONS)),'any':lambda x:np.nanmax(x.values,axis=0).astype(int) if x.shape[0]>0 else np.zeros(len(SPHERE_LOCATIONS)),\n",
        "                     'count':lambda x:np.nansum(x.values>0,axis=0) if x.shape[0]>0 else np.zeros(len(SPHERE_LOCATIONS)),}\n",
        "        elif self.sensor_type==SensorType.RSSI:\n",
        "             num_aps_init=4; return {'mean':lambda x:np.nanmean(x,axis=0) if x.shape[0]>0 else np.full(num_aps_init,-100.0),'std':lambda x:np.nanstd(x,axis=0) if x.shape[0]>0 else np.zeros(num_aps_init),\n",
        "                                     'max':lambda x:np.nanmax(x,axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>0 else np.full(num_aps_init,-100.0),'min':lambda x:np.nanmin(x,axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>0 else np.full(num_aps_init,-100.0),\n",
        "                                     'diff':lambda x:np.nanmax(x,axis=0)-np.nanmin(x,axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>1 else np.zeros(num_aps_init),'stability':lambda x:1.0/(1.0+np.nanstd(x,axis=0)) if x.shape[0]>0 else np.zeros(num_aps_init)}\n",
        "        elif self.sensor_type==SensorType.VIDEO:\n",
        "             return {'center_mean':lambda x:np.nanmean(x[['centre_2d_x','centre_2d_y']].values,axis=0) if x.shape[0]>0 else np.zeros(2),'center_std':lambda x:np.nanstd(x[['centre_2d_x','centre_2d_y']].values,axis=0) if x.shape[0]>0 else np.zeros(2),\n",
        "                     'area_mean':lambda x:self._calculate_area(x,'mean') if x.shape[0]>0 else np.zeros(1),'area_std':lambda x:self._calculate_area(x,'std') if x.shape[0]>0 else np.zeros(1),\n",
        "                     'aspect_ratio_mean':lambda x:self._calculate_aspect_ratio(x,'mean') if x.shape[0]>0 else np.zeros(1),'movement_mean':lambda x:self._calculate_movement(x,'mean') if x.shape[0]>1 else np.zeros(1),\n",
        "                     'height_mean':lambda x:self._calculate_height(x,'mean') if x.shape[0]>0 else np.zeros(1),'y_center_mean':lambda x:np.nanmean(x['centre_2d_y'].values) if x.shape[0]>0 else np.zeros(1),}\n",
        "        else: return {'identity':lambda x:x}\n",
        "\n",
        "    def _calculate_fft_energy(self, data, sampling_rate):\n",
        "        energies=[]; n_samples=data.shape[0];\n",
        "        if n_samples==0: return np.zeros(data.shape[1])\n",
        "        for i in range(data.shape[1]):\n",
        "            col_data=data[:,i]; valid_data=col_data[~np.isnan(col_data)]; n_valid=len(valid_data)\n",
        "            if n_valid<2: energies.append(0); continue\n",
        "            fft_values=rfft(valid_data); fft_power=np.abs(fft_values)**2/n_valid; energies.append(np.sum(fft_power))\n",
        "        return np.array(energies)\n",
        "    def _calculate_fft_peak_freq(self, data, sampling_rate):\n",
        "        peak_freqs=[]; n_samples=data.shape[0];\n",
        "        if n_samples==0: return np.zeros(data.shape[1])\n",
        "        for i in range(data.shape[1]):\n",
        "            col_data=data[:,i]; valid_data=col_data[~np.isnan(col_data)]; n_valid=len(valid_data)\n",
        "            if n_valid<2 or sampling_rate<=0: peak_freqs.append(0); continue\n",
        "            freqs=rfftfreq(n_valid,1.0/sampling_rate); fft_values=rfft(valid_data); fft_power=np.abs(fft_values)**2; valid_freq_indices=np.arange(len(freqs))\n",
        "            if len(fft_power)>1: peak_index_rel=np.argmax(fft_power[1:]); peak_index_abs=valid_freq_indices[1:][peak_index_rel]; peak_freqs.append(freqs[peak_index_abs])\n",
        "            else: peak_freqs.append(0)\n",
        "        return np.array(peak_freqs)\n",
        "    def _calculate_area(self, x, stat='mean'):\n",
        "        req=['bb_2d_br_x','bb_2d_br_y','bb_2d_tl_x','bb_2d_tl_y'];\n",
        "        if not all(c in x.columns for c in req): return np.zeros(1)\n",
        "        w=x['bb_2d_br_x']-x['bb_2d_tl_x']; h=x['bb_2d_br_y']-x['bb_2d_tl_y']; a=w*h;\n",
        "        if a.isnull().all(): return np.zeros(1)\n",
        "        if stat=='mean': return np.array([np.nanmean(a)])\n",
        "        if stat=='std': return np.array([np.nanstd(a)])\n",
        "        return np.zeros(1)\n",
        "    def _calculate_aspect_ratio(self, x, stat='mean'):\n",
        "        req=['bb_2d_br_x','bb_2d_br_y','bb_2d_tl_x','bb_2d_tl_y'];\n",
        "        if not all(c in x.columns for c in req): return np.zeros(1)\n",
        "        w=x['bb_2d_br_x']-x['bb_2d_tl_x']; h=x['bb_2d_br_y']-x['bb_2d_tl_y']; v=(h!=0)&(~h.isnull())&(~w.isnull());\n",
        "        if not v.any(): return np.zeros(1)\n",
        "        r=pd.Series(np.nan,index=x.index); r.loc[v]=w.loc[v]/h.loc[v];\n",
        "        if r.isnull().all(): return np.zeros(1)\n",
        "        if stat=='mean': return np.array([np.nanmean(r)])\n",
        "        if stat=='std': return np.array([np.nanstd(r)])\n",
        "        return np.zeros(1)\n",
        "    def _calculate_movement(self, x, stat='mean'):\n",
        "        req=['centre_2d_x','centre_2d_y'];\n",
        "        if not all(c in x.columns for c in req) or x.shape[0]<2: return np.zeros(1)\n",
        "        cx=x['centre_2d_x']; cy=x['centre_2d_y']; dx=cx-cx.shift(1); dy=cy-cy.shift(1); m=np.sqrt(dx**2+dy**2);\n",
        "        if m.isnull().all(): return np.zeros(1)\n",
        "        if stat=='mean': return np.array([np.nanmean(m)])\n",
        "        if stat=='std': return np.array([np.nanstd(m)])\n",
        "        return np.zeros(1)\n",
        "    def _calculate_height(self, x, stat='mean'):\n",
        "        req=['bb_2d_br_y','bb_2d_tl_y'];\n",
        "        if not all(c in x.columns for c in req): return np.zeros(1)\n",
        "        h=x['bb_2d_br_y']-x['bb_2d_tl_y'];\n",
        "        if h.isnull().all(): return np.zeros(1)\n",
        "        if stat=='mean': return np.array([np.nanmean(h)])\n",
        "        if stat=='std': return np.array([np.nanstd(h)])\n",
        "        return np.zeros(1)\n",
        "\n",
        "    def process_reading(self, timestamp, data_chunk, ground_truth_activity=None):\n",
        "        self.last_timestamp=timestamp\n",
        "        if data_chunk is None or (isinstance(data_chunk,pd.DataFrame) and data_chunk.empty):\n",
        "             d_s={act:1.0/len(SPHERE_ACTIVITY_NAMES) for act in SPHERE_ACTIVITY_NAMES}; return {'agent_id':self.id,'timestamp':timestamp,'activity_scores':d_s,'confidence':0.1,'prediction':None,'features':None,'error':'Missing data'}\n",
        "        features=self._extract_features(data_chunk)\n",
        "        if not features:\n",
        "             d_s={act:1.0/len(SPHERE_ACTIVITY_NAMES) for act in SPHERE_ACTIVITY_NAMES}; return {'agent_id':self.id,'timestamp':timestamp,'activity_scores':d_s,'confidence':0.1,'prediction':None,'features':features,'error':'Feature extraction failed'}\n",
        "        activity_scores=self._determine_activity_scores(features)\n",
        "        if not activity_scores: pred_act=None\n",
        "        else: scores_noise={k:v+np.random.rand()*1e-9 for k,v in activity_scores.items()}; pred_act=max(scores_noise,key=scores_noise.get)\n",
        "        if ground_truth_activity is not None: self.update_knowledge(features,activity_scores,pred_act,ground_truth_activity)\n",
        "        self.history.append((timestamp,features,activity_scores,pred_act))\n",
        "        return {'agent_id':self.id,'timestamp':timestamp,'sensor_type':self.sensor_type,'activity_scores':activity_scores,'confidence':self.confidence,'activity_specialization':self.activity_specialization,'prediction':pred_act,'features':features}\n",
        "\n",
        "    def _extract_features(self, data_chunk):\n",
        "        features={}; req_cols_present=True\n",
        "        try:\n",
        "            if self.sensor_type==SensorType.ACCELEROMETER:\n",
        "                 req=['x','y','z'];\n",
        "                 if all(c in data_chunk.columns for c in req): data=data_chunk[req].values; [features.update({n:e(data)}) for n,e in self.feature_extractors.items()]\n",
        "                 else: req_cols_present=False\n",
        "            elif self.sensor_type==SensorType.PIR:\n",
        "                 req=SPHERE_LOCATIONS;\n",
        "                 if all(c in data_chunk.columns for c in req): data=data_chunk[req]; [features.update({n:e(data)}) for n,e in self.feature_extractors.items()]\n",
        "                 else: req_cols_present=False\n",
        "            elif self.sensor_type==SensorType.RSSI:\n",
        "                 cols=[c for c in data_chunk.columns if c.startswith('AP') or 'rssi' in c.lower()];\n",
        "                 if cols and all(c in data_chunk.columns for c in cols): data=data_chunk[cols].apply(pd.to_numeric,errors='coerce').values; self.feature_extractors=self._update_num_aps(data.shape[1]); [features.update({n:e(data)}) for n,e in self.feature_extractors.items()]\n",
        "                 else: req_cols_present=False\n",
        "            elif self.sensor_type==SensorType.VIDEO:\n",
        "                 req=['centre_2d_x','centre_2d_y','bb_2d_br_x','bb_2d_br_y','bb_2d_tl_x','bb_2d_tl_y'];\n",
        "                 if all(c in data_chunk.columns for c in req): data=data_chunk[req].apply(pd.to_numeric,errors='coerce'); [features.update({n:e(data)}) for n,e in self.feature_extractors.items()]\n",
        "                 else: req_cols_present=False\n",
        "            if not req_cols_present: return {}\n",
        "            flat_features={}\n",
        "            for n,v in features.items():\n",
        "                if isinstance(v,(np.ndarray,list)):\n",
        "                    valid=[val for val in (v[~np.isnan(v)] if isinstance(v,np.ndarray) else [i for i in v if pd.notna(i)])]\n",
        "                    if len(valid)>0:\n",
        "                        if len(v)>1: [flat_features.update({f\"{n}_{i}\":val if pd.notna(val) else 0}) for i,val in enumerate(v)]\n",
        "                        elif len(v)==1: flat_features[n]=v[0] if pd.notna(v[0]) else 0\n",
        "                elif pd.notna(v): flat_features[n]=v\n",
        "            return flat_features\n",
        "        except Exception as e: print(f\"Error extracting features for Agent {self.id} ({self.sensor_type}): {e}\"); traceback.print_exc(); return {}\n",
        "\n",
        "    def _update_num_aps(self, num_found):\n",
        "        if self.sensor_type==SensorType.RSSI:\n",
        "             num_aps=num_found;\n",
        "             try: cur_dim=len(self.feature_extractors['mean'](np.array([[]])))\n",
        "             except: cur_dim=-1\n",
        "             if num_aps!=cur_dim and num_aps>0:\n",
        "                 print(f\"  Agent {self.id}: Updating RSSI feature extractors for {num_aps} APs.\")\n",
        "                 return {'mean':lambda x:np.nanmean(x,axis=0) if x.shape[0]>0 else np.full(num_aps,-100.0),'std':lambda x:np.nanstd(x,axis=0) if x.shape[0]>0 else np.zeros(num_aps),\n",
        "                         'max':lambda x:np.nanmax(x,axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>0 else np.full(num_aps,-100.0),'min':lambda x:np.nanmin(x,axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>0 else np.full(num_aps,-100.0),\n",
        "                         'diff':lambda x:np.nanmax(x,axis=0)-np.nanmin(x,axis=0) if x.shape[0]>0 and np.sum(~np.isnan(x))>1 else np.zeros(num_aps),'stability':lambda x:1.0/(1.0+np.nanstd(x,axis=0)) if x.shape[0]>0 else np.zeros(num_aps)}\n",
        "        return self.feature_extractors\n",
        "\n",
        "    def _determine_activity_scores(self, features):\n",
        "        if not features: return {act:1.0/len(SPHERE_ACTIVITY_NAMES) for act in SPHERE_ACTIVITY_NAMES}\n",
        "        scores=self.knowledge.copy(); base_mult=1.0\n",
        "        try:\n",
        "            if self.sensor_type==SensorType.ACCELEROMETER:\n",
        "                std_s=features.get('std_0',0)+features.get('std_1',0)+features.get('std_2',0); en_s=features.get('energy_0',0)+features.get('energy_1',0)+features.get('energy_2',0)\n",
        "                if std_s>1.5 or en_s>1.0:\n",
        "                    for act,type in SPHERE_ACTIVITIES.items():\n",
        "                        if type==ActivityType.AMBULATION: scores[act]*=(base_mult*1.5)\n",
        "                        elif type==ActivityType.TRANSITION: scores[act]*=(base_mult*1.2)\n",
        "                        elif type==ActivityType.POSTURE: scores[act]*=(base_mult*0.5)\n",
        "                    if features.get('fft_peak_freq_2',0)>1.5 and features.get('range_2',0)>5.0: scores['a_jump']*=1.5\n",
        "                    elif features.get('std_0',0)>1.0 or features.get('std_1',0)>1.0: scores['a_walk']*=1.2; scores['a_loadwalk']*=1.2\n",
        "                elif std_s<0.3 and en_s<0.2:\n",
        "                     for act,type in SPHERE_ACTIVITIES.items():\n",
        "                        if type==ActivityType.POSTURE: scores[act]*=(base_mult*1.5)\n",
        "                        elif type==ActivityType.AMBULATION: scores[act]*=(base_mult*0.5)\n",
        "                     mz=features.get('mean_2',0); my=features.get('mean_1',0); mx=features.get('mean_0',0); mag=np.sqrt(mx**2+my**2+mz**2); nx,ny,nz=(mx/mag,my/mag,mz/mag) if mag>1e-6 else (0,0,0)\n",
        "                     if nz>0.8: scores['p_stand']*=1.4\n",
        "                     elif abs(nx)>0.7 or abs(ny)>0.7: scores['p_lie']*=1.4\n",
        "                     else: scores['p_sit']*=1.3\n",
        "                else:\n",
        "                    for act,type in SPHERE_ACTIVITIES.items():\n",
        "                         if type==ActivityType.TRANSITION: scores[act]*=(base_mult*1.4)\n",
        "                         elif type==ActivityType.POSTURE: scores[act]*=(base_mult*0.8)\n",
        "                         elif type==ActivityType.AMBULATION: scores[act]*=(base_mult*0.8)\n",
        "            elif self.sensor_type==SensorType.PIR:\n",
        "                tc=sum(features.get(f'count_{i}',0) for i in range(len(SPHERE_LOCATIONS))); na=sum(features.get(f'any_{i}',0) for i in range(len(SPHERE_LOCATIONS)))\n",
        "                if tc>3 or na>1:\n",
        "                    for act,type in SPHERE_ACTIVITIES.items():\n",
        "                         if type==ActivityType.AMBULATION: scores[act]*=(base_mult*1.4)\n",
        "                         elif type==ActivityType.POSTURE: scores[act]*=(base_mult*0.6)\n",
        "                elif tc==0:\n",
        "                     for act,type in SPHERE_ACTIVITIES.items():\n",
        "                         if type==ActivityType.POSTURE: scores[act]*=(base_mult*1.2)\n",
        "                         elif type==ActivityType.AMBULATION: scores[act]*=(base_mult*0.8)\n",
        "            elif self.sensor_type==SensorType.RSSI:\n",
        "                n_aps=len([k for k in features if k.startswith('std_')]); avg_std=np.mean([features.get(f'std_{i}',100) for i in range(n_aps)]) if n_aps>0 else 100\n",
        "                if avg_std>3.0:\n",
        "                     for act,type in SPHERE_ACTIVITIES.items():\n",
        "                        if type==ActivityType.AMBULATION: scores[act]*=(base_mult*1.3)\n",
        "                        elif type==ActivityType.POSTURE: scores[act]*=(base_mult*0.7)\n",
        "                elif avg_std<1.0:\n",
        "                     for act,type in SPHERE_ACTIVITIES.items():\n",
        "                        if type==ActivityType.POSTURE: scores[act]*=(base_mult*1.3)\n",
        "                        elif type==ActivityType.AMBULATION: scores[act]*=(base_mult*0.7)\n",
        "            elif self.sensor_type==SensorType.VIDEO:\n",
        "                asp=features.get('aspect_ratio_mean',1.0); mov=features.get('movement_mean',0.0); h=features.get('height_mean',0.0); yc=features.get('y_center_mean',0.0)\n",
        "                MVH=15.0; MVL=3.0; ASPT=0.6; ASPW=1.3; HT=150; HS=70; YL=300\n",
        "                if mov>MVH:\n",
        "                     for act,type in SPHERE_ACTIVITIES.items():\n",
        "                        if type==ActivityType.AMBULATION: scores[act]*=(base_mult*1.4)\n",
        "                        elif type==ActivityType.TRANSITION: scores[act]*=(base_mult*1.2)\n",
        "                        elif type==ActivityType.POSTURE: scores[act]*=(base_mult*0.6)\n",
        "                elif mov<MVL:\n",
        "                    for act,type in SPHERE_ACTIVITIES.items():\n",
        "                        if type==ActivityType.POSTURE: scores[act]*=(base_mult*1.4)\n",
        "                        elif type==ActivityType.AMBULATION: scores[act]*=(base_mult*0.6)\n",
        "                    if asp<ASPT and h>HT: scores['p_stand']*=1.5\n",
        "                    elif asp>ASPW and yc>YL: scores['p_lie']*=1.5\n",
        "                    elif h<HS and asp>0.8: scores['p_squat']*=1.2; scores['p_kneel']*=1.2\n",
        "                    else: scores['p_sit']*=1.3; scores['p_bent']*=1.2\n",
        "        except KeyError as e: print(f\"Warning: Agent {self.id} missing key: {e}\")\n",
        "        except Exception as e: print(f\"Error scoring Agent {self.id}: {e}\"); traceback.print_exc()\n",
        "        if self.history:\n",
        "            last_p=self.history[-1][3]\n",
        "            if last_p is not None and last_p in scores: scores[last_p]*=1.1\n",
        "        total=sum(scores.values())\n",
        "        if total>1e-9:\n",
        "            norm_s={act:max(0,s/total) for act,s in scores.items()}; final_t=sum(norm_s.values())\n",
        "            if final_t>1e-9: norm_s={act:s/final_t for act,s in norm_s.items()}\n",
        "            else: norm_s={act:1.0/len(SPHERE_ACTIVITY_NAMES) for act in SPHERE_ACTIVITY_NAMES}\n",
        "        else: norm_s={act:1.0/len(SPHERE_ACTIVITY_NAMES) for act in SPHERE_ACTIVITY_NAMES}\n",
        "        return norm_s\n",
        "\n",
        "    def update_knowledge(self, features, activity_scores, prediction, ground_truth):\n",
        "        alpha=self.learning_rate\n",
        "        if prediction==ground_truth:\n",
        "            self.confidence=(1-alpha)*self.confidence+alpha*1.0; self.activity_accuracy[prediction]=(1-alpha)*self.activity_accuracy[prediction]+alpha*1.0; self.activity_specialization[prediction]=(1-alpha)*self.activity_specialization[prediction]+alpha*1.0\n",
        "        else:\n",
        "            self.confidence=(1-alpha)*self.confidence+alpha*0.0\n",
        "            if prediction is not None: self.activity_accuracy[prediction]=(1-alpha)*self.activity_accuracy[prediction]+alpha*0.0; self.activity_specialization[prediction]=max(0.01,(1-alpha)*self.activity_specialization[prediction]+alpha*0.3)\n",
        "            self.activity_accuracy[ground_truth]=(1-alpha)*self.activity_accuracy[ground_truth]+alpha*0.3; self.activity_specialization[ground_truth]=max(0.01,(1-alpha)*self.activity_specialization[ground_truth]+alpha*0.4)\n",
        "        beta=alpha*0.5; cur_k_sum=sum(self.knowledge.values())\n",
        "        for act,score in activity_scores.items(): self.knowledge[act]=(1-beta)*self.knowledge[act]+beta*score*cur_k_sum\n",
        "        total_k=sum(self.knowledge.values())\n",
        "        if total_k>1e-9: self.knowledge={act:k/total_k for act,k in self.knowledge.items()}\n",
        "\n",
        "# --- SwarmQueen Integration System ---\n",
        "class SwarmQueen:\n",
        "    \"\"\"Manages the swarm of agents and integrates their outputs.\"\"\"\n",
        "    def __init__(self, agents, activity_names, location_names):\n",
        "        self.agents={a.id:a for a in agents}; self.activity_names=activity_names; self.location_names=location_names; self.num_activities=len(activity_names); self.system_history=deque(maxlen=20)\n",
        "        self.location_activity_prob=self._initialize_location_activity_map(); self.confusion_matrix=pd.DataFrame(np.zeros((self.num_activities,self.num_activities)),index=activity_names,columns=activity_names)\n",
        "        self.fusion_confidence_weight=1.0; self.fusion_specialization_weight=1.0; self.temporal_inertia_boost=1.1\n",
        "\n",
        "    def _initialize_location_activity_map(self):\n",
        "        loc_map={loc:{act:1.0/self.num_activities for act in self.activity_names} for loc in self.location_names+['UNKNOWN']}; base_p=1.0/self.num_activities\n",
        "        def update(loc,factors):\n",
        "            if loc not in loc_map: return\n",
        "            for act,f in factors.items():\n",
        "                 if act in loc_map[loc]: loc_map[loc][act]*=f\n",
        "            total=sum(loc_map[loc].values())\n",
        "            if total>1e-9: loc_map[loc]={act:p/total for act,p in loc_map[loc].items()}\n",
        "            else: loc_map[loc]={act:base_p for act in self.activity_names}\n",
        "        update('living',{'p_sit':4,'p_lie':2,'a_walk':1.5,'t_sit_stand':1.5,'t_stand_sit':1.5}); update('bed1',{'p_lie':5,'p_sit':2,'t_lie_sit':2,'t_sit_lie':2}); update('bed2',{'p_lie':5,'p_sit':2,'t_lie_sit':2,'t_sit_lie':2})\n",
        "        update('kitchen',{'p_stand':3,'a_walk':2,'p_bent':1.5}); update('bath',{'p_stand':3,'a_walk':1.5,'p_bent':1.5}); update('toilet',{'p_sit':5,'p_stand':1.5,'t_sit_stand':1.5,'t_stand_sit':1.5})\n",
        "        update('stairs',{'a_ascend':6,'a_descend':6,'p_stand':1.5,'a_walk':1.2}); update('hall',{'a_walk':4,'p_stand':2}); update('study',{'p_sit':5,'p_stand':1.5,'t_sit_stand':1.2,'t_stand_sit':1.2})\n",
        "        loc_map['UNKNOWN']={act:base_p for act in self.activity_names}; return loc_map\n",
        "\n",
        "    def process_system_reading(self, timestamp, data_chunks_by_sensortype, ground_truth_activity=None, ground_truth_location=None):\n",
        "        agent_outs={}; agent_locs={}\n",
        "        for agent_id,agent in self.agents.items():\n",
        "            chunk=data_chunks_by_sensortype.get(agent.sensor_type,None); gt=ground_truth_activity\n",
        "            output=agent.process_reading(timestamp,chunk,ground_truth_activity=gt); agent_outs[agent_id]=output\n",
        "            if output and 'error' not in output:\n",
        "                if agent.sensor_type==SensorType.PIR and output.get('features'):\n",
        "                    any_p=[output['features'].get(f'any_{i}',0) for i in range(len(self.location_names))]\n",
        "                    if sum(any_p)==1: idx=np.argmax(any_p); agent_locs[agent_id]={'location':self.location_names[idx],'confidence':agent.confidence}\n",
        "        fused=self._weighted_fusion(agent_outs); inferred_loc=self._determine_location(agent_locs,ground_truth_location)\n",
        "        context=self._apply_location_context(fused,inferred_loc); final_s=self._apply_temporal_consistency(context)\n",
        "        if not final_s: final_p=None\n",
        "        else: noise_s={k:v+np.random.rand()*1e-9 for k,v in final_s.items()}; final_p=max(noise_s,key=noise_s.get)\n",
        "        self.system_history.append((timestamp,final_p,inferred_loc,final_s))\n",
        "        if ground_truth_activity is not None and final_p is not None: self.update_confusion_matrix(ground_truth_activity,final_p)\n",
        "        return {'timestamp':timestamp,'final_prediction':final_p,'final_scores':final_s,'inferred_location':inferred_loc,'agent_outputs':agent_outs}\n",
        "\n",
        "    def _determine_location(self, agent_locations, ground_truth_location=None):\n",
        "         if not agent_locations: return \"UNKNOWN\"\n",
        "         votes=defaultdict(float); total_conf=0.0\n",
        "         for agent_id,info in agent_locations.items():\n",
        "             conf=info.get('confidence',0.1); loc=info.get('location','UNKNOWN')\n",
        "             if loc!='UNKNOWN': votes[loc]+=conf; total_conf+=conf\n",
        "         if not votes or total_conf<0.1:\n",
        "            if self.system_history: last_loc=self.system_history[-1][2]; return last_loc if last_loc!=\"UNKNOWN\" else \"UNKNOWN\"\n",
        "            return \"UNKNOWN\"\n",
        "         return max(votes,key=votes.get)\n",
        "\n",
        "    def _weighted_fusion(self, agent_outputs):\n",
        "        fused={act:0.0 for act in self.activity_names}; total_w={act:1e-9 for act in self.activity_names}\n",
        "        for agent_id,output in agent_outputs.items():\n",
        "            if not output or 'error' in output or not output.get('activity_scores'): continue\n",
        "            conf=output.get('confidence',0.1); spec=output.get('activity_specialization',{act:0.5 for act in self.activity_names}); scores=output['activity_scores']\n",
        "            for act,score in scores.items():\n",
        "                if act not in fused: continue\n",
        "                ct=max(0.01,conf)**self.fusion_confidence_weight; st=max(0.01,spec.get(act,0.5))**self.fusion_specialization_weight; w=ct*st\n",
        "                fused[act]+=score*w; total_w[act]+=w\n",
        "        for act in fused: fused[act]/=total_w[act]\n",
        "        final_t=sum(fused.values())\n",
        "        if final_t>1e-9: fused={act:s/final_t for act,s in fused.items()}\n",
        "        else: fused={act:1.0/self.num_activities for act in self.activity_names}\n",
        "        return fused\n",
        "\n",
        "    def _apply_location_context(self, scores, location):\n",
        "        if location not in self.location_activity_prob: location=\"UNKNOWN\"\n",
        "        context={}; probs=self.location_activity_prob[location]\n",
        "        for act,score in scores.items(): prior=probs.get(act,1.0/self.num_activities); context[act]=score*prior\n",
        "        total=sum(context.values())\n",
        "        if total>1e-9: context={act:s/total for act,s in context.items()}\n",
        "        else: context={act:1.0/self.num_activities for act in self.activity_names}\n",
        "        return context\n",
        "\n",
        "    def _apply_temporal_consistency(self, current_scores):\n",
        "        if not self.system_history: return current_scores\n",
        "        last_p=self.system_history[-1][1]; smoothed=current_scores.copy()\n",
        "        if last_p is not None and last_p in smoothed: smoothed[last_p]*=self.temporal_inertia_boost\n",
        "        total=sum(smoothed.values())\n",
        "        if total>1e-9: smoothed={act:s/total for act,s in smoothed.items()}\n",
        "        else: smoothed={act:1.0/self.num_activities for act in self.activity_names}\n",
        "        return smoothed\n",
        "\n",
        "    def update_confusion_matrix(self, gt, pred):\n",
        "        if gt in self.confusion_matrix.index and pred in self.confusion_matrix.columns: self.confusion_matrix.loc[gt,pred]+=1\n",
        "\n",
        "    def get_confusion_matrix(self): return self.confusion_matrix\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Initializing SPHERE Swarm Intelligence System...\")\n",
        "    # San Diego Time: Monday, March 31, 2025 at 4:43 AM PDT\n",
        "    print(f\"Current Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # --- Configuration ---\n",
        "    DATA_PATH = \"/content/SPHERE_unzipped\" # <<< --- VERIFY YOUR UNZIPPED DATA PATH --- <<<\n",
        "    TARGET_SUBJECT = \"00001\" # <<<--- USE CORRECT SUBJECT ID (e.g., \"00001\") ---<<<\n",
        "    WINDOW_SIZE_SEC = 2.0\n",
        "    OVERLAP_SEC = 1.0\n",
        "    RESAMPLE_FREQ = '50ms' # Corresponds to 20 Hz\n",
        "\n",
        "    # --- Initialize Agents ---\n",
        "    agents_to_use = [\n",
        "        Agent(agent_id=\"Accel_01\", sensor_type=SensorType.ACCELEROMETER, learning_rate=0.05, history_len=10),\n",
        "        Agent(agent_id=\"PIR_01\", sensor_type=SensorType.PIR, learning_rate=0.1, history_len=5),\n",
        "    ]\n",
        "    print(f\"Initialized {len(agents_to_use)} agents.\")\n",
        "\n",
        "    # --- Initialize SwarmQueen ---\n",
        "    queen = SwarmQueen(agents_to_use, SPHERE_ACTIVITY_NAMES, SPHERE_LOCATIONS)\n",
        "    print(\"Initialized SwarmQueen.\")\n",
        "\n",
        "    # --- Data Loading ---\n",
        "    print(\"\\n--- Starting Data Loading and Processing ---\")\n",
        "    print(f\"Target Subject: {TARGET_SUBJECT}\")\n",
        "    print(f\"Attempting to load data from: {DATA_PATH}\")\n",
        "    accel_data = load_sphere_data(DATA_PATH, TARGET_SUBJECT, 'accelerometer')\n",
        "    pir_data = load_sphere_data(DATA_PATH, TARGET_SUBJECT, 'pir')\n",
        "    annotations = load_sphere_data(DATA_PATH, TARGET_SUBJECT, 'annotations')\n",
        "\n",
        "    # --- Robustness Check ---\n",
        "    if accel_data is None or annotations is None:\n",
        "         print(\"\\n--- ERROR: Failed to load essential Accelerometer or Annotation data. Stopping. ---\")\n",
        "    else:\n",
        "        print(\"\\n--- Essential data (Accel, Annotations) loaded successfully. Proceeding... ---\")\n",
        "        if pir_data is None: print(\"--- Warning: Failed to load PIR data correctly. Proceeding without it. ---\")\n",
        "        else: print(\"--- PIR data loaded successfully. ---\")\n",
        "\n",
        "        all_data_streams = { SensorType.ACCELEROMETER: accel_data, SensorType.PIR: pir_data }\n",
        "        sync_data = synchronize_data(all_data_streams, resample_freq=RESAMPLE_FREQ)\n",
        "\n",
        "        if sync_data is None or sync_data.empty: print(\"\\n--- ERROR: Data synchronization failed. Stopping. ---\")\n",
        "        else:\n",
        "            window_generator = create_time_windows(sync_data, WINDOW_SIZE_SEC, OVERLAP_SEC)\n",
        "            print(\"Sorting annotations index...\")\n",
        "            if 'end' not in annotations.columns: print(\"Error: Annotations missing 'end' column.\"); exit()\n",
        "            annotations = annotations.sort_index()\n",
        "\n",
        "            # --- Processing Loop ---\n",
        "            print(\"\\n--- Processing Windows ---\")\n",
        "            all_predictions=[]; all_ground_truths=[]; window_count=0; start_processing_time=time.time()\n",
        "            debug_gt_lookup = True # Keep debugging prints enabled for now\n",
        "            # ------------------------------------\n",
        "            for window in window_generator:\n",
        "                window_count+=1; window_end_time=window['timestamp']; window_data_chunks=window['data']\n",
        "                ground_truth=None\n",
        "                try: # Ground Truth Lookup\n",
        "                    window_start_time=window_end_time-pd.Timedelta(seconds=WINDOW_SIZE_SEC); window_mid_point=window_start_time+(window_end_time-window_start_time)/2\n",
        "\n",
        "                    # --- DEBUG PRINTS START ---\n",
        "                    if debug_gt_lookup and (window_count <= 5 or window_count % 100 == 1):\n",
        "                         print(f\"\\n  [DEBUG GT W{window_count}] Window: [{window_start_time}, {window_end_time})\")\n",
        "                         print(f\"  [DEBUG GT W{window_count}] Midpoint: {window_mid_point}\")\n",
        "                    # --- DEBUG PRINTS END ---\n",
        "\n",
        "                    match_condition = (annotations.index <= window_mid_point) & (annotations['end'] > window_mid_point)\n",
        "                    matching_annotations = annotations.loc[match_condition]\n",
        "\n",
        "                    if debug_gt_lookup and (window_count <= 5 or window_count % 100 == 1):\n",
        "                         print(f\"  [DEBUG GT W{window_count}] Match Condition Sum: {match_condition.sum()}\")\n",
        "\n",
        "                    if not matching_annotations.empty:\n",
        "                        matched_annotation = matching_annotations.iloc[0]\n",
        "\n",
        "                        if debug_gt_lookup and (window_count <= 5 or window_count % 100 == 1):\n",
        "                             print(f\"  [DEBUG GT W{window_count}] Matched Annotation Row (Index={matched_annotation.name}):\\n{matched_annotation.to_string()}\")\n",
        "                             print(f\"  [DEBUG GT W{window_count}] Available columns in annotations df: {annotations.columns.tolist()}\")\n",
        "\n",
        "                        # --- <<< FIX: Add 'name' to the list of possible activity columns >>> ---\n",
        "                        activity_col = next((col for col in ['activity', 'annotation', 'label', 'name'] if col in annotations.columns), None) # Check for 'name' now\n",
        "\n",
        "                        if debug_gt_lookup and (window_count <= 5 or window_count % 100 == 1):\n",
        "                             print(f\"  [DEBUG GT W{window_count}] Activity Col Found?: {activity_col}\") # Should now find 'name'\n",
        "\n",
        "                        if activity_col:\n",
        "                            ground_truth = matched_annotation[activity_col]\n",
        "                            if debug_gt_lookup and (window_count <= 5 or window_count % 100 == 1):\n",
        "                                 print(f\"  [DEBUG GT W{window_count}] Ground Truth Set To: '{ground_truth}'\") # Should show the activity\n",
        "                        # else: # This case should be less likely now\n",
        "                            # print(f\"  [DEBUG GT W{window_count}] Could not find activity column\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error looking up annotation for window ending at {window_end_time} (midpoint {window_mid_point}): {e}\")\n",
        "                    traceback.print_exc()\n",
        "                # --- End Ground Truth Lookup ---\n",
        "\n",
        "                system_output=queen.process_system_reading(window_end_time,window_data_chunks,ground_truth_activity=ground_truth)\n",
        "                if ground_truth is not None and ground_truth in SPHERE_ACTIVITY_NAMES: all_ground_truths.append(ground_truth); all_predictions.append(system_output['final_prediction'])\n",
        "                # Print normal progress (less frequently now maybe)\n",
        "                if window_count % 200 == 0: print(f\"  Processed {window_count} windows... Time: {window_end_time.strftime('%H:%M:%S.%f')[:-3]} Pred: {system_output['final_prediction']} (GT: {ground_truth if ground_truth else 'N/A'}) Loc: {system_output['inferred_location']}\")\n",
        "            # --- End Window Processing Loop ---\n",
        "\n",
        "            end_processing_time=time.time(); print(f\"\\n--- Finished processing {window_count} windows in {end_processing_time-start_processing_time:.2f} seconds ---\")\n",
        "\n",
        "            # --- Evaluation ---\n",
        "            print(\"\\n--- Evaluation Results ---\"); final_confusion_matrix=queen.get_confusion_matrix(); print(f\"Collected {len(all_ground_truths)} predictions with valid ground truth.\")\n",
        "            if len(all_ground_truths) > 0 and len(all_predictions) == len(all_ground_truths):\n",
        "                accuracy=accuracy_score(all_ground_truths,all_predictions); print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "                unique_labels=sorted(list(set(all_ground_truths)|set(all_predictions))); unique_labels=[label for label in unique_labels if label is not None and label in SPHERE_ACTIVITY_NAMES]\n",
        "                print(\"\\nClassification Report:\");\n",
        "                try: report=classification_report(all_ground_truths,all_predictions,labels=unique_labels,zero_division=0); print(report)\n",
        "                except ValueError as e: print(f\"Could not generate classification report: {e}\")\n",
        "                print(\"\\nFinal Confusion Matrix:\"); cm_labels=sorted(list(set(all_ground_truths)|set(all_predictions))); cm_labels=[l for l in cm_labels if l is not None and l in final_confusion_matrix.index and l in final_confusion_matrix.columns]\n",
        "                if cm_labels:\n",
        "                    cm_display=final_confusion_matrix.loc[cm_labels,cm_labels]; print(cm_display)\n",
        "                    try:\n",
        "                        plt.figure(figsize=(12,10)); sns.heatmap(cm_display,annot=True,fmt=\".0f\",cmap=\"Blues\",xticklabels=cm_labels,yticklabels=cm_labels); plt.title(f'System Confusion Matrix (Subject: {TARGET_SUBJECT})')\n",
        "                        plt.ylabel('True Label'); plt.xlabel('Predicted Label'); plt.xticks(rotation=45,ha='right'); plt.yticks(rotation=0); plt.tight_layout()\n",
        "                        plot_filename=f\"confusion_matrix_{TARGET_SUBJECT}.png\"; plt.savefig(plot_filename); print(f\"\\nConfusion matrix plot saved as {plot_filename}\"); plt.close()\n",
        "                    except Exception as plot_err: print(f\"Error plotting confusion matrix: {plot_err}\")\n",
        "                else: print(\"\\nCould not generate confusion matrix (no common valid labels found).\")\n",
        "            else:\n",
        "                 if len(all_ground_truths)==0: print(\"\\nEvaluation failed: No valid ground truth labels were found.\"); print(\"Check annotation loading and GT lookup logic (including timestamps/units and activity column name).\")\n",
        "                 else: print(\"\\nEvaluation failed: Length mismatch between predictions and ground truth lists.\")\n",
        "\n",
        "    print(\"\\nSystem processing finished (or terminated due to loading/sync error).\")\n",
        "\n"
      ]
    }
  ]
}